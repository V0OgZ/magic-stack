#!/bin/bash
# Script de d√©marrage des LLM exp√©rimentaux pour Clippy Mode Dev

set -e

# Couleurs
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m'

echo -e "${BLUE}‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ${NC}"
echo -e "${BLUE}ü§ñ LLM EXPERIMENTAL LAUNCHER - Clippy Mode Dev${NC}"
echo -e "${BLUE}‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ${NC}"
echo ""

# Configuration des ports
OLLAMA_PORT=11434      # Ollama API
LLAMA_CPP_PORT=8888    # llama.cpp (si install√©)
CLIPPY_DEV_PORT=8889   # Notre wrapper Clippy Dev

# Fonction pour v√©rifier si un port est libre
check_port() {
    if lsof -Pi :$1 -sTCP:LISTEN -t >/dev/null 2>&1; then
        return 1
    else
        return 0
    fi
}

# Fonction pour tuer un processus sur un port
kill_port() {
    local port=$1
    local pids=$(lsof -ti:$port 2>/dev/null)
    if [ ! -z "$pids" ]; then
        echo -e "${YELLOW}Arr√™t du processus sur port $port...${NC}"
        kill -9 $pids 2>/dev/null || true
        sleep 1
    fi
}

# Options du menu
case "${1:-menu}" in
    start)
        echo -e "${GREEN}üì¶ D√©marrage des services LLM...${NC}"
        echo ""
        
        # 1. OLLAMA
        echo -e "${BLUE}1. Ollama (port $OLLAMA_PORT)${NC}"
        if check_port $OLLAMA_PORT; then
            echo "   Lancement d'Ollama..."
            OLLAMA_HOST=0.0.0.0:$OLLAMA_PORT ollama serve > logs/ollama.log 2>&1 &
            sleep 2
            
            # T√©l√©charger les mod√®les si n√©cessaire
            echo "   V√©rification des mod√®les..."
            if ! ollama list 2>/dev/null | grep -q "qwen2.5:0.5b"; then
                echo "   T√©l√©chargement de Qwen2.5:0.5b (397MB)..."
                ollama pull qwen2.5:0.5b
            fi
            
            if ! ollama list 2>/dev/null | grep -q "tinyllama"; then
                echo "   T√©l√©chargement de TinyLlama (637MB)..."
                ollama pull tinyllama
            fi
            
            echo -e "   ${GREEN}‚úì Ollama d√©marr√©${NC}"
        else
            echo -e "   ${YELLOW}! Ollama d√©j√† actif${NC}"
        fi
        
        # 2. Wrapper Clippy Dev
        echo ""
        echo -e "${BLUE}2. Clippy Dev Server (port $CLIPPY_DEV_PORT)${NC}"
        if check_port $CLIPPY_DEV_PORT; then
            echo "   Lancement du serveur Clippy Dev..."
            python3 clippy_dev_server.py > logs/clippy_dev.log 2>&1 &
            echo -e "   ${GREEN}‚úì Clippy Dev d√©marr√©${NC}"
        else
            echo -e "   ${YELLOW}! Clippy Dev d√©j√† actif${NC}"
        fi
        
        echo ""
        echo -e "${GREEN}‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ${NC}"
        echo -e "${GREEN}‚úÖ Services LLM d√©marr√©s !${NC}"
        echo ""
        echo "üìç Endpoints disponibles :"
        echo "   ‚Ä¢ Ollama API    : http://localhost:$OLLAMA_PORT"
        echo "   ‚Ä¢ Clippy Dev    : http://localhost:$CLIPPY_DEV_PORT"
        echo ""
        echo "üß™ Test rapide :"
        echo "   curl http://localhost:$CLIPPY_DEV_PORT/ask -d '{\"q\":\"What is Heroes of Time?\"}'"
        echo -e "${GREEN}‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ${NC}"
        ;;
        
    stop)
        echo -e "${RED}üõë Arr√™t des services LLM...${NC}"
        kill_port $OLLAMA_PORT
        kill_port $CLIPPY_DEV_PORT
        kill_port $LLAMA_CPP_PORT
        echo -e "${GREEN}‚úì Services arr√™t√©s${NC}"
        ;;
        
    status)
        echo -e "${BLUE}üìä Status des services LLM${NC}"
        echo ""
        
        # Ollama
        if check_port $OLLAMA_PORT; then
            echo -e "Ollama ($OLLAMA_PORT)    : ${RED}‚ö´ Arr√™t√©${NC}"
        else
            echo -e "Ollama ($OLLAMA_PORT)    : ${GREEN}üü¢ Actif${NC}"
            echo "   Mod√®les disponibles :"
            ollama list 2>/dev/null | grep -E "qwen|llama|phi" | sed 's/^/      /' || echo "      (aucun)"
        fi
        
        # Clippy Dev
        if check_port $CLIPPY_DEV_PORT; then
            echo -e "Clippy Dev ($CLIPPY_DEV_PORT) : ${RED}‚ö´ Arr√™t√©${NC}"
        else
            echo -e "Clippy Dev ($CLIPPY_DEV_PORT) : ${GREEN}üü¢ Actif${NC}"
        fi
        
        echo ""
        ;;
        
    test)
        echo -e "${BLUE}üß™ Test des LLM${NC}"
        echo ""
        
        # Test Ollama direct
        echo "1. Test Ollama direct..."
        curl -s http://localhost:$OLLAMA_PORT/api/generate \
            -d '{"model":"qwen2.5:0.5b","prompt":"Say hello in 5 words","stream":false}' \
            | jq -r '.response' 2>/dev/null || echo "   ‚ùå Ollama non disponible"
        
        echo ""
        echo "2. Test Clippy Dev..."
        curl -s http://localhost:$CLIPPY_DEV_PORT/ask \
            -H "Content-Type: application/json" \
            -d '{"q":"What is temporal drift?"}' \
            | jq -r '.answer' 2>/dev/null || echo "   ‚ùå Clippy Dev non disponible"
        
        echo ""
        ;;
        
    models)
        echo -e "${BLUE}üì¶ Gestion des mod√®les${NC}"
        echo ""
        echo "Mod√®les install√©s :"
        ollama list 2>/dev/null || echo "  (Ollama non actif)"
        echo ""
        echo "Pour installer un mod√®le :"
        echo "  ollama pull qwen2.5:0.5b    # Ultra-l√©ger (397MB)"
        echo "  ollama pull tinyllama        # L√©ger (637MB)"
        echo "  ollama pull phi3.5           # Plus puissant (2.2GB)"
        ;;
        
    logs)
        echo -e "${BLUE}üìú Logs des services${NC}"
        echo ""
        echo "Ollama :"
        tail -n 20 logs/ollama.log 2>/dev/null || echo "  (pas de logs)"
        echo ""
        echo "Clippy Dev :"
        tail -n 20 logs/clippy_dev.log 2>/dev/null || echo "  (pas de logs)"
        ;;
        
    *)
        echo "Usage: ./llm [commande]"
        echo ""
        echo "Commandes disponibles :"
        echo "  start   - D√©marre tous les services LLM"
        echo "  stop    - Arr√™te tous les services"
        echo "  status  - Affiche le status"
        echo "  test    - Test les endpoints"
        echo "  models  - G√®re les mod√®les"
        echo "  logs    - Affiche les logs"
        echo ""
        echo "Ports utilis√©s :"
        echo "  ‚Ä¢ $OLLAMA_PORT  : Ollama API"
        echo "  ‚Ä¢ $CLIPPY_DEV_PORT : Clippy Dev Server"
        echo "  ‚Ä¢ $LLAMA_CPP_PORT : llama.cpp (futur)"
        ;;
esac
